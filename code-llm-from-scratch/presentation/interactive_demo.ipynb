{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation Model: Interactive Tutorial\n",
    "\n",
    "This notebook walks you through building and using a code generation model, from **foundational concepts to advanced usage**.\n",
    "\n",
    "**Learning Path**:\n",
    "1. üî∞ Foundations: Understanding tokenization\n",
    "2. üèóÔ∏è Architecture: Building the transformer\n",
    "3. üéì Training: Two-stage training process\n",
    "4. üöÄ Generation: Creating code from prompts\n",
    "5. üéØ Advanced: Fine-tuning and optimization\n",
    "\n",
    "**Duration**: 60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add src to path\nimport sys\nfrom pathlib import Path\n\n# Set project root (presentation folder is inside the project)\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root))\n\n# Imports from src modules\nimport torch\nimport torch.nn.functional as F\nfrom src.tokenizer import BPETokenizer, Vocabulary\nfrom src.model import CodeTransformer, CoderConfig\nfrom src.training import CodeTrainer, create_dataloaders\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"‚úì Imports successful\")\nprint(f\"‚úì PyTorch version: {torch.__version__}\")\nprint(f\"‚úì Device: {'MPS' if torch.backends.mps.is_available() else 'CPU'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 1: Foundations üî∞\n\n## 1.1 Understanding Tokenization\n\n**Question**: How do we convert text into numbers that a neural network can process?\n\n**Answer**: Tokenization!\n\n![Tokenization Process](../docs/diagrams/tokenization-process.svg)\n\nLet's explore different tokenization strategies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "sample_text = \"#!/bin/bash\\nfor i in {1..10}; do\\n    echo $i\\ndone\"\n",
    "\n",
    "print(\"Original bash script:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Character-level tokenization\n",
    "char_tokens = list(sample_text)\n",
    "print(f\"\\nCharacter tokens ({len(char_tokens)} tokens):\")\n",
    "print(char_tokens[:20], \"...\")\n",
    "\n",
    "# Word-level tokenization (naive)\n",
    "word_tokens = sample_text.split()\n",
    "print(f\"\\nWord tokens ({len(word_tokens)} tokens):\")\n",
    "print(word_tokens)\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(f\"Characters: {len(char_tokens)} tokens, vocab size ~256\")\n",
    "print(f\"Words: {len(word_tokens)} tokens, vocab size ~50,000+\")\n",
    "print(f\"BPE: ~30 tokens (optimal!), vocab size ~8,000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why BPE (Byte Pair Encoding)?\n",
    "\n",
    "BPE finds the **sweet spot** between character and word tokenization:\n",
    "- Common words/commands ‚Üí single token\n",
    "- Rare words ‚Üí split into subwords\n",
    "- No out-of-vocabulary issues!\n",
    "\n",
    "Let's see BPE in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a simple BPE tokenizer\n",
    "tokenizer = BPETokenizer()\n",
    "\n",
    "# Sample training data\n",
    "training_texts = [\n",
    "    \"#!/bin/bash\",\n",
    "    \"for i in {1..10}; do\",\n",
    "    \"echo 'Hello World'\",\n",
    "    \"if [ -f file.txt ]; then\",\n",
    "    \"grep -r 'pattern' /path\",\n",
    "]\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer.target_vocab_size = 500\n",
    "tokenizer.train(training_texts, verbose=False)\n",
    "\n",
    "print(f\"‚úì Tokenizer trained with vocab size: {len(tokenizer.vocab)}\")\n",
    "print(f\"\\nExample tokenization:\")\n",
    "\n",
    "test_text = \"#!/bin/bash\\necho 'test'\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"Original: {repr(test_text)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: {repr(decoded)}\")\n",
    "print(f\"Match: {test_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Analysis\n",
    "\n",
    "Let's analyze what the tokenizer learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some learned tokens\n",
    "print(\"Sample vocabulary (first 30 tokens):\")\n",
    "for i, token in enumerate(list(tokenizer.vocab.token_to_id.keys())[:30]):\n",
    "    token_id = tokenizer.vocab.token_to_id[token]\n",
    "    print(f\"{token_id:3d}: {repr(token):20s}\", end=\"  \")\n",
    "    if (i + 1) % 3 == 0:\n",
    "        print()\n",
    "\n",
    "# Visualize token frequency\n",
    "print(\"\\n\\nüìä Vocabulary distribution:\")\n",
    "vocab_sizes = [len(token) for token in tokenizer.vocab.token_to_id.keys()]\n",
    "plt.hist(vocab_sizes, bins=20, edgecolor='black')\n",
    "plt.xlabel('Token Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Token Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 2: Architecture üèóÔ∏è\n\n## 2.1 The Transformer Model\n\nOur model is a **GPT-style transformer** with:\n- 6 layers\n- 384 hidden dimensions\n- 6 attention heads\n- 48.7M parameters\n\n![Transformer Architecture](../docs/diagrams/transformer-architecture.svg)\n\nLet's build it step by step!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "config = CoderConfig(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_layers=6,\n",
    "    d_model=384,\n",
    "    n_heads=6,\n",
    "    d_ff=1536,\n",
    "    max_seq_len=512,\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size:,}\")\n",
    "print(f\"  Layers: {config.n_layers}\")\n",
    "print(f\"  Hidden size: {config.d_model}\")\n",
    "print(f\"  Attention heads: {config.n_heads}\")\n",
    "print(f\"  Feed-forward size: {config.d_ff}\")\n",
    "print(f\"  Max sequence length: {config.max_seq_len}\")\n",
    "\n",
    "# Create model\n",
    "model = CodeTransformer(config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / (1024**2):.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parameter Breakdown\n",
    "\n",
    "Where are all those parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze parameter distribution\n",
    "param_counts = {}\n",
    "for name, param in model.named_parameters():\n",
    "    component = name.split('.')[0]\n",
    "    if component not in param_counts:\n",
    "        param_counts[component] = 0\n",
    "    param_counts[component] += param.numel()\n",
    "\n",
    "# Visualize\n",
    "components = list(param_counts.keys())\n",
    "counts = list(param_counts.values())\n",
    "percentages = [c / total_params * 100 for c in counts]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(components, percentages, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Percentage of Parameters')\n",
    "plt.title('Parameter Distribution in Transformer')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for i, (comp, pct, count) in enumerate(zip(components, percentages, counts)):\n",
    "    plt.text(i, pct + 1, f'{pct:.1f}%\\n({count/1e6:.1f}M)', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Parameter Breakdown:\")\n",
    "for comp, count, pct in zip(components, counts, percentages):\n",
    "    print(f\"  {comp:20s}: {count/1e6:6.2f}M ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Understanding Self-Attention\n\nThe **key innovation** of transformers is self-attention. Let's visualize how it works!\n\n![Attention Mechanism](../docs/diagrams/attention-mechanism.svg)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple attention visualization\n",
    "def visualize_attention(text, tokenizer):\n",
    "    \"\"\"Visualize attention pattern for a simple example.\"\"\"\n",
    "    tokens_ids = tokenizer.encode(text)\n",
    "    tokens_text = [tokenizer.vocab.id_to_token.get(tid, '<UNK>') for tid in tokens_ids]\n",
    "    \n",
    "    # Create a simple attention matrix (causal)\n",
    "    seq_len = len(tokens_ids)\n",
    "    attention = np.tril(np.random.rand(seq_len, seq_len))\n",
    "    \n",
    "    # Normalize rows\n",
    "    attention = attention / attention.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention, \n",
    "                xticklabels=tokens_text,\n",
    "                yticklabels=tokens_text,\n",
    "                cmap='YlOrRd',\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    plt.xlabel('Key (what to attend to)')\n",
    "    plt.ylabel('Query (current token)')\n",
    "    plt.title('Causal Self-Attention Pattern')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attention\n",
    "\n",
    "# Example\n",
    "example_text = \"for i in range\"\n",
    "print(f\"Visualizing attention for: '{example_text}'\\n\")\n",
    "attention_matrix = visualize_attention(example_text, tokenizer)\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"  - Diagonal: Each token attends to itself\")\n",
    "print(\"  - Lower triangle: Can only attend to previous tokens (causal)\")\n",
    "print(\"  - Brighter = stronger attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 3: Training üéì\n\n## 3.1 Two-Stage Training Process\n\nModern code models use **two-stage training**:\n\n![Two-Stage Training](../docs/diagrams/two-stage-training.svg)\n\n```\nStage 1: Language Pretraining\n  Data: Natural language (TinyStories)\n  Goal: Learn grammar, vocabulary, reasoning\n  Duration: 2-4 hours\n\nStage 2: Code Fine-Tuning  \n  Data: Code (100+ bash scripts)\n  Goal: Learn code syntax and patterns\n  Duration: 30-60 minutes\n```\n\nLet's simulate a mini training run!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini dataset for demonstration\n",
    "demo_texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A dog ran in the park.\",\n",
    "    \"The sun shines brightly.\",\n",
    "    \"Birds fly in the sky.\",\n",
    "    \"Children play at school.\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "demo_tokens = [tokenizer.encode(text) for text in demo_texts]\n",
    "\n",
    "print(\"Demo Training Data:\")\n",
    "for text, tokens in zip(demo_texts, demo_tokens):\n",
    "    print(f\"  '{text}' ‚Üí {len(tokens)} tokens\")\n",
    "\n",
    "# Create tiny model for quick demo\n",
    "tiny_config = CoderConfig(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_layers=2,  # Fewer layers\n",
    "    d_model=128,  # Smaller\n",
    "    n_heads=4,\n",
    "    d_ff=512,\n",
    "    max_seq_len=128,\n",
    ")\n",
    "\n",
    "tiny_model = CodeTransformer(tiny_config)\n",
    "device = torch.device('cpu')  # Use CPU for demo\n",
    "tiny_model = tiny_model.to(device)\n",
    "\n",
    "print(f\"\\nTiny model: {sum(p.numel() for p in tiny_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 Training Loop (Simplified)\n\nLet's run a few training steps to see the loss decrease!\n\n![Training Loop](../docs/diagrams/training-loop.svg)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(tiny_model.parameters(), lr=1e-3)\n",
    "losses = []\n",
    "\n",
    "# Prepare data\n",
    "max_len = max(len(t) for t in demo_tokens)\n",
    "padded_tokens = [t + [0] * (max_len - len(t)) for t in demo_tokens]\n",
    "input_tensor = torch.tensor(padded_tokens, dtype=torch.long, device=device)\n",
    "\n",
    "print(\"Training for 50 steps...\\n\")\n",
    "\n",
    "# Training loop\n",
    "tiny_model.train()\n",
    "for step in range(50):\n",
    "    # Forward pass\n",
    "    logits, loss = tiny_model(input_tensor, targets=input_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track loss\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step {step+1:2d}/50: loss = {loss.item():.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress (Demo)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Loss decreased from {losses[0]:.4f} to {losses[-1]:.4f}\")\n",
    "print(\"  Model is learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 4: Generation üöÄ\n\n## 4.1 Loading a Trained Model\n\nNow let's load a fully trained model and generate code!\n\n![Generation Process](../docs/diagrams/generation-process.svg)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if trained model exists\nimport os\n\n# Model paths (relative to project root)\nmodel_path = project_root / \"models\" / \"code\" / \"code_model_final.pt\"\ntokenizer_path = project_root / \"models\" / \"language\" / \"language_tokenizer.json\"\n\nif model_path.exists():\n    print(\"‚úì Found trained model!\")\n    print(f\"  Model: {model_path}\")\n    print(f\"  Tokenizer: {tokenizer_path}\")\n    print(\"\\nLoading...\")\n    \n    # Load tokenizer\n    production_tokenizer = BPETokenizer()\n    production_tokenizer.load(str(tokenizer_path))\n    \n    # Load model\n    checkpoint = torch.load(model_path, map_location='cpu')\n    production_config = checkpoint.get('config', CoderConfig(vocab_size=len(production_tokenizer.vocab)))\n    production_model = CodeTransformer(production_config)\n    production_model.load_state_dict(checkpoint['model_state_dict'])\n    production_model.eval()\n    \n    print(f\"\\n‚úì Model loaded ({sum(p.numel() for p in production_model.parameters()):,} parameters)\")\n    has_trained_model = True\nelse:\n    print(\"‚ö† No trained model found.\")\n    print(f\"  Expected: {model_path}\")\n    print(\"\\n  To train a model, run:\")\n    print(\"    python scripts/train_language.py\")\n    print(\"    python scripts/train_code.py\")\n    print(\"\\nUsing untrained model for demonstration...\")\n    production_model = tiny_model\n    production_tokenizer = tokenizer\n    has_trained_model = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Code Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(prompt, max_length=200, temperature=0.8, top_k=50):\n",
    "    \"\"\"Generate code from a prompt.\"\"\"\n",
    "    print(f\"Prompt: {repr(prompt)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = production_tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long)\n",
    "    \n",
    "    # Generate\n",
    "    production_model.eval()\n",
    "    generated = input_tensor.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_length):\n",
    "            # Get predictions\n",
    "            logits, _ = production_model(generated)\n",
    "            next_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Top-k sampling\n",
    "            if top_k > 0:\n",
    "                indices_to_remove = next_logits < torch.topk(next_logits, top_k)[0][..., -1, None]\n",
    "                next_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append\n",
    "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Stop at reasonable length for bash scripts\n",
    "            if i > 50 and next_token.item() == production_tokenizer.vocab.token_to_id.get('\\n', -1):\n",
    "                break\n",
    "    \n",
    "    # Decode\n",
    "    output = production_tokenizer.decode(generated[0].tolist())\n",
    "    print(output)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generate Your First Script!\n",
    "\n",
    "Try different prompts and see what the model generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Backup script\n",
    "prompt1 = \"#!/bin/bash\\n# Create a backup script for MySQL\"\n",
    "output1 = generate_code(prompt1, max_length=150, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: System monitoring\n",
    "prompt2 = \"#!/bin/bash\\n# Monitor system resources\"\n",
    "output2 = generate_code(prompt2, max_length=150, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Your custom prompt!\n",
    "custom_prompt = \"#!/bin/bash\\n# \"  # Add your description here\n",
    "output3 = generate_code(custom_prompt, max_length=150, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Temperature Effects\n",
    "\n",
    "Let's see how temperature affects generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"#!/bin/bash\\necho \"\n",
    "\n",
    "print(\"Low Temperature (0.3) - Conservative:\")\n",
    "generate_code(prompt, max_length=50, temperature=0.3)\n",
    "\n",
    "print(\"\\nMedium Temperature (0.8) - Balanced:\")\n",
    "generate_code(prompt, max_length=50, temperature=0.8)\n",
    "\n",
    "print(\"\\nHigh Temperature (1.5) - Creative:\")\n",
    "generate_code(prompt, max_length=50, temperature=1.5)\n",
    "\n",
    "print(\"\\nüí° Observation:\")\n",
    "print(\"  Low temp ‚Üí More predictable, safer\")\n",
    "print(\"  High temp ‚Üí More varied, riskier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Advanced Topics üéØ\n",
    "\n",
    "## 5.1 Token Probability Analysis\n",
    "\n",
    "Let's peek inside the model to see what it's thinking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(prompt, top_k=10):\n",
    "    \"\"\"Show top-k next token predictions.\"\"\"\n",
    "    print(f\"Analyzing: {repr(prompt)}\\n\")\n",
    "    \n",
    "    # Encode\n",
    "    input_ids = production_tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits, _ = production_model(input_tensor)\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # Get top-k\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    print(f\"Top {top_k} most likely next tokens:\\n\")\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = production_tokenizer.vocab.id_to_token.get(idx.item(), '<UNK>')\n",
    "        print(f\"  {prob.item()*100:5.2f}% ‚Üí {repr(token)}\")\n",
    "\n",
    "# Analyze what comes after \"#!/bin/bash\"\n",
    "analyze_predictions(\"#!/bin/bash\\n#\", top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Comparing Different Architectures\n",
    "\n",
    "How does model size affect quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model sizes\n",
    "sizes = {\n",
    "    'Tiny': {'n_layers': 4, 'd_model': 256, 'n_heads': 4, 'd_ff': 1024},\n",
    "    'Small': {'n_layers': 6, 'd_model': 384, 'n_heads': 6, 'd_ff': 1536},\n",
    "    'Medium': {'n_layers': 12, 'd_model': 768, 'n_heads': 12, 'd_ff': 3072},\n",
    "}\n",
    "\n",
    "size_comparison = []\n",
    "\n",
    "for name, params in sizes.items():\n",
    "    config = CoderConfig(\n",
    "        vocab_size=8000,\n",
    "        max_seq_len=512,\n",
    "        **params\n",
    "    )\n",
    "    model = CodeTransformer(config)\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    size_comparison.append({\n",
    "        'Size': name,\n",
    "        'Parameters (M)': param_count / 1e6,\n",
    "        'Layers': params['n_layers'],\n",
    "        'Hidden': params['d_model'],\n",
    "        'Heads': params['n_heads'],\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(size_comparison)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Visualize parameter scaling\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(df['Size'], df['Parameters (M)'], color=['lightblue', 'skyblue', 'steelblue'])\n",
    "plt.ylabel('Parameters (Millions)')\n",
    "plt.title('Model Size Comparison')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    plt.text(i, row['Parameters (M)'] + 5, f\"{row['Parameters (M)']:.1f}M\", \n",
    "             ha='center', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Trade-offs:\")\n",
    "print(\"  Tiny: Fast, low memory, lower quality\")\n",
    "print(\"  Small: Balanced (recommended)\")\n",
    "print(\"  Medium: Best quality, slower, more memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training Data Impact\n",
    "\n",
    "Let's visualize our training data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Load bash scripts statistics\nstats_path = project_root / \"data\" / \"code\" / \"bash_scripts\" / \"stats.json\"\nif stats_path.exists():\n    with open(stats_path, 'r') as f:\n        stats = json.load(f)\n    \n    print(\"Training Data Statistics:\")\n    print(f\"  Scripts: {stats['num_scripts']}\")\n    print(f\"  Lines: {stats['total_lines']:,}\")\n    print(f\"  Characters: {stats['total_chars']:,}\")\n    print(f\"  Avg lines/script: {stats['avg_lines']:.1f}\")\n    print(f\"  Avg chars/script: {stats['avg_chars']:.1f}\")\n    \n    # Visualize categories\n    categories = {\n        'System Admin': 20,\n        'DevOps/CI': 20,\n        'Database': 15,\n        'Networking': 15,\n        'Monitoring': 15,\n        'Deployment': 15,\n    }\n    \n    plt.figure(figsize=(10, 6))\n    plt.pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%',\n            startangle=90, colors=sns.color_palette('Set3'))\n    plt.title('Training Data Distribution by Category')\n    plt.axis('equal')\n    plt.show()\nelse:\n    print(f\"Stats file not found: {stats_path}\")\n    print(\"Run: python scripts/generate_bash_dataset.py\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Summary and Next Steps\n\n## What We Learned\n\n1. **‚úì Tokenization**: BPE balances vocab size and sequence length\n2. **‚úì Architecture**: Transformers use self-attention for context\n3. **‚úì Training**: Two-stage (language + code) is most efficient\n4. **‚úì Generation**: Temperature controls creativity vs. correctness\n5. **‚úì Scaling**: Bigger models = better quality but more resources\n\n## Visual References\n\nAll diagrams used in this notebook are in `docs/diagrams/`:\n- `tokenization-process.svg` - How tokenization works\n- `transformer-architecture.svg` - Model structure\n- `attention-mechanism.svg` - Self-attention explained\n- `two-stage-training.svg` - Training pipeline\n- `training-loop.svg` - Training process\n- `generation-process.svg` - Code generation\n\n## Try These Next\n\n```python\n# 1. Generate different script types\nprompts = [\n    \"#!/bin/bash\\n# Deployment script\",\n    \"#!/bin/bash\\n# Log analyzer\",\n    \"#!/bin/bash\\n# Network monitor\",\n]\n\n# 2. Experiment with generation parameters\ngenerate_code(prompt, temperature=0.5, top_k=20)\n\n# 3. Fine-tune on your own bash scripts\n# See: examples/fine_tuning.py\n```\n\n## Resources\n\n- **Architecture Guide**: `docs/ARCHITECTURE.md`\n- **Deployment Guide**: `docs/DEPLOYMENT.md`\n- **Advanced Topics**: `docs/ADVANCED_TOPICS.md`\n- **Visual Guide**: `docs/VISUAL_GUIDE.md`\n- **Presentation Guide**: `presentation/PRESENTATION_GUIDE.md`\n\n---\n\n**üéâ Congratulations!** You now understand how modern code generation models work!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}