# Docker Compose configuration for Code LLM API
#
# Usage:
#   docker-compose up -d        # Start in background
#   docker-compose logs -f      # View logs
#   docker-compose down         # Stop
#   docker-compose restart      # Restart

version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    image: code-llm-api:latest
    container_name: code-llm-api
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=models/code/code_model_final.pt
      - TOKENIZER_PATH=models/language/language_tokenizer.json
      - MAX_PROMPT_LENGTH=200
      - MAX_GENERATION_LENGTH=500
      - DEFAULT_TEMPERATURE=0.8
      - DEFAULT_TOP_K=50
      - DEFAULT_TOP_P=0.9
    volumes:
      # Mount models directory if you want to update models without rebuilding
      - ./models:/app/models:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - code-llm-network

  # Optional: Nginx reverse proxy
  # Uncomment if you want to add caching and load balancing
  # nginx:
  #   image: nginx:alpine
  #   container_name: code-llm-nginx
  #   ports:
  #     - "80:80"
  #   volumes:
  #     - ./nginx.conf:/etc/nginx/nginx.conf:ro
  #   depends_on:
  #     - api
  #   networks:
  #     - code-llm-network

networks:
  code-llm-network:
    driver: bridge
