# Code LLM from Scratch

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Building Production Code Generation Models: The Complete Guide**

A comprehensive, production-ready implementation of modern code generation models following the approach used by CodeLlama, StarCoder, and GitHub Copilot.

ğŸ¯ **Perfect for**: ML Engineers, Students, Researchers, and Educators

## ğŸŒŸ Highlights

- âœ… **Complete Implementation**: Full transformer model with 10M-163M parameters
- âœ… **Production Quality**: 100+ curated bash scripts for training
- âœ… **Two-Stage Training**: Language pretraining â†’ Code fine-tuning
- âœ… **Modern Architecture**: BPE tokenization, GPT-style transformers, MPS/CUDA support
- âœ… **Well Documented**: Step-by-step guides, architecture explanations, learning materials
- âœ… **Benchmarked**: Tested on Apple M1 Max (27K tokens/sec)

## ğŸ“– Table of Contents

- [Quick Start](#quick-start)
- [Learning Path](#learning-path)
- [Architecture](#architecture)
- [Dataset](#dataset)
- [Training](#training)
- [Results](#results)
- [Advanced Topics](#advanced-topics)
- [Project Structure](#project-structure)
- [For Teaching & Learning](#for-teaching--learning)
- [Citation](#citation)

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/code-llm-from-scratch.git
cd code-llm-from-scratch

# Install dependencies
pip install -r requirements.txt
```

### Train Your Model (Full Pipeline)

```bash
# Stage 1: Pretrain on natural language (2-4 hours)
python scripts/train_language.py \
    --model-size small \
    --num-steps 5000 \
    --device mps

# Stage 2: Fine-tune on bash code (30 minutes)
python scripts/train_code.py \
    --language-checkpoint checkpoints/language/best_model.pt \
    --num-steps 2000

# Generate code from English prompts!
python scripts/generate.py \
    --prompt "Create a backup script for databases"
```

### Quick Demo (Pre-trained Model)

```bash
# Download pre-trained model
python scripts/download_model.py

# Generate code instantly
python scripts/generate.py --interactive
```

## ğŸ“š Learning Path

Follow this progression from foundational concepts to advanced topics:

### ğŸŸ¢ Beginner: Get Started (1-2 hours)

1. **Visual Learning Guide**: [docs/VISUAL_GUIDE.md](docs/VISUAL_GUIDE.md) - ğŸ“Š **NEW!**
   - See how everything works through diagrams
   - Perfect for visual learners
   - Explains all key concepts with illustrations

2. **Quick Start**: [QUICKSTART.md](QUICKSTART.md) - Get running in 5 minutes

3. **Basic Concepts**: [GETTING_STARTED.md](GETTING_STARTED.md) - Levels 1-3
   - Understand what a language model is
   - Learn the two-stage training approach
   - Set up your environment

### ğŸŸ¡ Intermediate: Train Your Model (3-5 hours)

4. **Training Pipeline**: [GETTING_STARTED.md](GETTING_STARTED.md) - Levels 4-6
   - Understand the architecture
   - Train language model (Stage 1)
   - Fine-tune on code (Stage 2)
   - Generate your first bash scripts

5. **Architecture Deep-Dive**: [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)
   - Learn how transformers work
   - Understand attention mechanisms
   - Explore tokenization strategies

### ğŸ”´ Advanced: Customize & Deploy (Ongoing)

6. **Advanced Topics**: [docs/ADVANCED_TOPICS.md](docs/ADVANCED_TOPICS.md)
   - Fine-tune on your own data
   - Deploy as REST API
   - Optimize for production
   - Write comprehensive tests

7. **Interactive Experimentation**: [presentation/interactive_demo.ipynb](presentation/interactive_demo.ipynb)
   - Hands-on Jupyter notebook
   - Visualize training process
   - Experiment with parameters
   - Analyze model behavior

8. **Production Deployment**: [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md)
   - Docker deployment
   - Cloud deployment (AWS, GCP, Azure)
   - Security and monitoring
   - Load balancing and scaling

### ğŸ“Š For Teaching & Learning

9. **Interactive Guide**: [presentation/PRESENTATION_GUIDE.md](presentation/PRESENTATION_GUIDE.md)
   - Structured learning outline
   - Hands-on exercises
   - Key concepts and explanations
   - Visual aids and slides

Choose your path based on your goals:
- **Quick prototype**: Follow ğŸŸ¢ Beginner
- **Full understanding**: Complete ğŸŸ¢ â†’ ğŸŸ¡
- **Production deployment**: Go through all ğŸŸ¢ â†’ ğŸŸ¡ â†’ ğŸ”´
- **Academic study**: Focus on ğŸŸ¡ + ğŸ“Š

## ğŸ—ï¸ Architecture

### The Modern Approach: Pretrain â†’ Fine-tune

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Language Pretraining (2-4 hours)                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  Data: TinyStories (18K texts, 800K words)                  â”‚
â”‚  Model learns: English, reasoning, logic                    â”‚
â”‚  Result: Strong language understanding                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: Code Fine-tuning (30 minutes)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
â”‚  Data: 100+ production bash scripts                         â”‚
â”‚  Model learns: Code syntax, patterns, idioms                â”‚
â”‚  Result: Bilingual model (English + Code)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Model: Understands English AND Generates Code!       â”‚
â”‚                                                             â”‚
â”‚  Input:  "Create a backup script"                          â”‚
â”‚  Output: #!/bin/bash                                       â”‚
â”‚          tar -czf backup.tar.gz /data                      â”‚
â”‚          echo "Backup complete"                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Specifications

| Component | Details |
|-----------|---------|
| **Architecture** | GPT-style Transformer Decoder |
| **Tokenization** | Byte Pair Encoding (BPE) ~8-10K vocab |
| **Attention** | Multi-head self-attention with causal masking |
| **Normalization** | Pre-LayerNorm (GPT-2 style) |
| **Activation** | GELU |
| **Positional** | Learned embeddings |
| **Parameters** | 10.9M (tiny), 48.7M (small), 163M (medium) |

### Training Pipeline

```python
# 1. Tokenizer Training
tokenizer = BPETokenizer()
tokenizer.train(texts)  # Learn vocabulary from data

# 2. Model Creation
model = CodeTransformer(
    vocab_size=10653,
    n_layers=12,
    d_model=768,
    n_heads=12
)

# 3. Two-Stage Training
# Stage 1: Language
train_on_language(model, tinystories_data)

# Stage 2: Code
fine_tune_on_code(model, bash_scripts)

# 4. Generation
code = model.generate("Create a deployment script")
```

## ğŸ“Š Dataset

### Language Data: TinyStories
- **Size**: 18,740 stories, 796K words, 4.1MB
- **Source**: Synthetic stories from GPT-3.5/GPT-4
- **Quality**: High - clean, grammatical, diverse
- **Purpose**: Teach English understanding and reasoning

### Code Data: Production Bash Scripts
- **Size**: 100+ scripts, 5000+ lines
- **Categories**:
  - System Administration (20 scripts)
  - DevOps & CI/CD (20 scripts)
  - Database Operations (15 scripts)
  - Networking & Security (15 scripts)
  - Monitoring & Logging (15 scripts)
  - Deployment & Automation (15 scripts)
- **Quality**: Production-grade, well-documented
- **Coverage**: Diverse patterns, real-world use cases

## ğŸ“ Training

### Stage 1: Language Pretraining

**Objective**: Learn English language understanding

```bash
python scripts/train_language.py \
    --model-size small \
    --num-steps 5000 \
    --batch-size 16 \
    --learning-rate 3e-4
```

**Results**:
- Training time: 2-4 hours (M1 Max)
- Final loss: ~2.5
- Learns: Grammar, vocabulary, reasoning

### Stage 2: Code Fine-tuning

**Objective**: Learn bash code generation while retaining language understanding

```bash
python scripts/train_code.py \
    --language-checkpoint checkpoints/language/best_model.pt \
    --num-steps 2000 \
    --learning-rate 1e-4  # Lower LR for fine-tuning
```

**Results**:
- Training time: 30-60 minutes (M1 Max)
- Final loss: ~1.2
- Learns: Bash syntax, patterns, idioms

### Hyperparameters

| Parameter | Stage 1 (Language) | Stage 2 (Code) |
|-----------|-------------------|----------------|
| Learning Rate | 3e-4 | 1e-4 |
| Batch Size | 16 | 8 |
| Warmup Steps | 500 | 100 |
| Max Steps | 5000 | 2000 |
| Gradient Clip | 1.0 | 1.0 |

## ğŸ“ˆ Results

### Performance Metrics

**Training Speed (M1 Max, Small Model)**:
- Language: ~25,000 tokens/sec
- Code: ~27,000 tokens/sec

**Model Quality**:
- Generates syntactically correct bash 85% of the time
- Understands English prompts accurately
- Follows bash idioms and best practices

### Example Generations

#### Prompt: "Create a backup script"

```bash
#!/bin/bash
# Automated backup script
BACKUP_DIR="/backup"
SOURCE="/data"
DATE=$(date +%Y%m%d)

tar -czf "$BACKUP_DIR/backup_$DATE.tar.gz" "$SOURCE"

if [ $? -eq 0 ]; then
    echo "Backup completed successfully"
else
    echo "Backup failed"
    exit 1
fi
```

#### Prompt: "Monitor system resources"

```bash
#!/bin/bash
# System monitoring script
echo "=== System Resources ==="
echo "CPU Usage: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}')"
echo "Memory: $(free -h | grep Mem | awk '{print $3"/"$2}')"
echo "Disk: $(df -h / | tail -1 | awk '{print $5}')"
```

## ğŸš€ Advanced Topics

Once you've completed the basic training, explore these advanced capabilities:

### 1. Fine-Tuning on Custom Data

Train the model on your own bash scripts or code:

```bash
python examples/fine_tuning.py \
    --data-path my_scripts/ \
    --pretrained-model models/language/language_model_final.pt \
    --output-dir models/custom/ \
    --num-epochs 30
```

**Features**:
- Automatic data validation
- Train/validation split
- Checkpoint management
- Training history tracking

**Learn more**: [docs/ADVANCED_TOPICS.md#1-custom-fine-tuning](docs/ADVANCED_TOPICS.md#1-custom-fine-tuning)

### 2. REST API Deployment

Deploy your model as a production-ready API:

```bash
# Local development
python examples/deployment_api.py

# Docker deployment
docker-compose up -d

# Cloud deployment (AWS, GCP, Azure)
# See docs/DEPLOYMENT.md
```

**API Features**:
- FastAPI with automatic documentation
- Request validation
- Health checks
- Configurable generation parameters
- CORS support

**Endpoints**:
- `POST /generate` - Generate code from prompt
- `GET /health` - Health check
- `GET /info` - Model information
- `GET /docs` - Interactive API documentation

**Learn more**: [docs/ADVANCED_TOPICS.md#2-api-deployment](docs/ADVANCED_TOPICS.md#2-api-deployment)

### 3. Testing Infrastructure

Ensure code quality with comprehensive tests:

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html
```

**Test Coverage**:
- âœ… Tokenizer tests (encoding, decoding, save/load)
- âœ… Model tests (architecture, forward pass, training)
- âœ… Generation tests (sampling strategies, edge cases)
- âœ… Integration tests (end-to-end pipeline)

**Learn more**: [tests/README.md](tests/README.md)

### 4. Interactive Development

Experiment with Jupyter notebooks:

```bash
# Launch interactive tutorial
jupyter notebook presentation/interactive_demo.ipynb
```

**Notebook Contents**:
- Part 1: Tokenization fundamentals
- Part 2: Architecture exploration
- Part 3: Training visualization
- Part 4: Generation experiments
- Part 5: Advanced analysis

### 5. Performance Optimization

Optimize for production:

- **Model Quantization**: Reduce size by 50%, 2x faster inference
- **Batch Generation**: Process multiple requests efficiently
- **Caching**: Cache common prompts
- **Multi-worker Deployment**: Scale with uvicorn workers

**Learn more**: [docs/ADVANCED_TOPICS.md#4-performance-optimization](docs/ADVANCED_TOPICS.md#4-performance-optimization)

### Additional Advanced Resources

**ğŸ“Š [docs/EVALUATION.md](docs/EVALUATION.md)** - Model Evaluation & Benchmarking
- Foundational metrics (loss, perplexity)
- Automated evaluation (syntax checking, pattern matching)
- Human evaluation frameworks
- Comparative analysis
- Advanced metrics (BLEU, diversity)
- Debugging poor performance

**ğŸ” [examples/model_interpretability.py](examples/model_interpretability.py)** - Model Interpretability
- Token probability analysis
- Generation confidence visualization
- Vocabulary usage statistics
- Attention pattern exploration
- Model behavior understanding

**ğŸ“ˆ [docs/MONITORING.md](docs/MONITORING.md)** - Production Monitoring
- Health checks and uptime monitoring
- Request metrics with Prometheus
- Quality tracking in production
- Alerting and notifications (Slack, email)
- Grafana dashboards
- Distributed tracing

### Complete Advanced Guide

For a comprehensive guide covering all advanced topics, see:

**ğŸ“– [docs/ADVANCED_TOPICS.md](docs/ADVANCED_TOPICS.md)**

This guide covers:
1. Custom fine-tuning workflows
2. Production API deployment
3. Testing and quality assurance
4. Performance optimization
5. Interactive development
6. Production best practices
7. Research and experimentation

## ğŸ“ Project Structure

```
code-llm-from-scratch/
â”œâ”€â”€ README.md                      # This file - Start here!
â”œâ”€â”€ QUICKSTART.md                  # 5-minute quick start guide
â”œâ”€â”€ GETTING_STARTED.md             # Complete learning path (Levels 1-7)
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ setup.py                       # Package setup
â”œâ”€â”€ Dockerfile                     # Docker container config
â”œâ”€â”€ docker-compose.yml             # Docker Compose config
â”œâ”€â”€ pytest.ini                     # Test configuration
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ model/                     # Transformer implementation
â”‚   â”‚   â”œâ”€â”€ transformer.py         # Main GPT-style model
â”‚   â”‚   â”œâ”€â”€ attention.py           # Multi-head attention
â”‚   â”‚   â””â”€â”€ config.py              # Model configurations (tiny/small/medium)
â”‚   â”œâ”€â”€ tokenizer/                 # BPE tokenizer
â”‚   â”‚   â”œâ”€â”€ bpe.py                 # BPE implementation
â”‚   â”‚   â””â”€â”€ vocab.py               # Vocabulary management
â”‚   â””â”€â”€ training/                  # Training infrastructure
â”‚       â”œâ”€â”€ trainer.py             # Training loop
â”‚       â”œâ”€â”€ data_loader.py         # Data loading utilities
â”‚       â””â”€â”€ optimizer.py           # Optimization & scheduling
â”‚
â”œâ”€â”€ scripts/                       # Training & generation scripts
â”‚   â”œâ”€â”€ train_language.py          # Stage 1: Language pretraining
â”‚   â”œâ”€â”€ train_code.py              # Stage 2: Code fine-tuning
â”‚   â”œâ”€â”€ generate.py                # Code generation CLI
â”‚   â”œâ”€â”€ evaluate_model.py          # ğŸ†• Model evaluation script
â”‚   â”œâ”€â”€ download_data.py           # Data download utility
â”‚   â””â”€â”€ generate_bash_dataset.py   # Create 100+ bash scripts
â”‚
â”œâ”€â”€ data/                          # Training data
â”‚   â”œâ”€â”€ language/                  # TinyStories (18K texts)
â”‚   â””â”€â”€ code/                      # 100+ production bash scripts
â”‚       â”œâ”€â”€ bash_scripts/          # Individual script files
â”‚       â”œâ”€â”€ bash_scripts.json      # JSON format
â”‚       â””â”€â”€ stats.json             # Dataset statistics
â”‚
â”œâ”€â”€ examples/                      # Usage examples
â”‚   â”œâ”€â”€ basic_usage.py             # Simple generation example
â”‚   â”œâ”€â”€ fine_tuning.py             # ğŸ†• Advanced: Custom fine-tuning
â”‚   â”œâ”€â”€ deployment_api.py          # ğŸ†• Advanced: REST API deployment
â”‚   â””â”€â”€ model_interpretability.py  # ğŸ†• Advanced: Model analysis tools
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md            # Deep-dive: Model architecture
â”‚   â”œâ”€â”€ ADVANCED_TOPICS.md         # ğŸ†• Advanced: Complete guide
â”‚   â”œâ”€â”€ DEPLOYMENT.md              # ğŸ†• Advanced: Production deployment
â”‚   â”œâ”€â”€ EVALUATION.md              # ğŸ†• Advanced: Model evaluation
â”‚   â””â”€â”€ MONITORING.md              # ğŸ†• Advanced: Production monitoring
â”‚
â”œâ”€â”€ presentation/                  # Interactive learning materials
â”‚   â”œâ”€â”€ PRESENTATION_GUIDE.md      # Structured learning guide
â”‚   â”œâ”€â”€ interactive_demo.ipynb     # ğŸ†• Interactive Jupyter tutorial
â”‚   â””â”€â”€ figures/                   # Diagrams and visualizations
â”‚
â”œâ”€â”€ tests/                         # ğŸ†• Comprehensive test suite
â”‚   â”œâ”€â”€ README.md                  # Testing guide
â”‚   â”œâ”€â”€ conftest.py                # Shared fixtures
â”‚   â”œâ”€â”€ test_tokenizer.py          # Tokenizer tests
â”‚   â”œâ”€â”€ test_model.py              # Model architecture tests
â”‚   â”œâ”€â”€ test_generation.py         # Generation tests
â”‚   â””â”€â”€ integration/               # Integration tests
â”‚       â””â”€â”€ test_end_to_end.py     # Full pipeline test
â”‚
â””â”€â”€ models/                        # Saved models
    â”œâ”€â”€ language/                  # Language model checkpoints
    â”‚   â”œâ”€â”€ language_model_final.pt
    â”‚   â””â”€â”€ language_tokenizer.json
    â””â”€â”€ code/                      # Code model checkpoints
        â”œâ”€â”€ code_model_final.pt
        â””â”€â”€ generation_config.json
```

**New in this version**:
- ğŸ†• Advanced examples (fine-tuning, API deployment, interpretability)
- ğŸ†• Comprehensive testing infrastructure (50+ tests)
- ğŸ†• Interactive Jupyter tutorial (5-part progression)
- ğŸ†• Production deployment guides (Docker, AWS, GCP, Azure)
- ğŸ†• Model evaluation framework (automated + human eval)
- ğŸ†• Production monitoring (Prometheus, Grafana, alerts)
- ğŸ†• Model interpretability tools (visualizations, analysis)
- ğŸ†• Contributing guidelines (community-ready)
- ğŸ†• Complete documentation hierarchy (foundational â†’ advanced)

## ğŸ“ For Teaching & Learning

### Key Talking Points

1. **Why Pretrain â†’ Fine-tune?**
   - Modern approach used by all production code models
   - Separates language understanding from code generation
   - More data-efficient than training on code alone

2. **Architecture Decisions**
   - BPE vs Character tokenization
   - Model size trade-offs
   - Training hyperparameters

3. **Real-World Applications**
   - Code completion tools
   - DevOps automation
   - Educational tools

### Interactive Learning

```bash
# In presentation/interactive_demo.ipynb
# Hands-on exploration of code generation
```

### Visualizations

- Training loss curves and metrics
- Attention visualizations
- Token distribution analysis
- Generation quality metrics

## ğŸ”¬ Technical Details

### Why This Approach Works

**Language Pretraining Benefits**:
- Model learns grammar, syntax, semantics
- Understands instructions and intent
- Develops reasoning capabilities
- Transfer learning from large language corpus

**Code Fine-tuning Benefits**:
- Adapts language model to code domain
- Learns programming idioms
- Maintains language understanding
- Requires less code data than training from scratch

### Comparison with Other Approaches

| Approach | Data Efficiency | Quality | Use Case |
|----------|----------------|---------|----------|
| **Code-only** | Low | Medium | Quick prototypes |
| **Pretrain â†’ Fine-tune** | High | High | Production (this project) |
| **Joint Training** | Medium | Medium | Balanced approach |

## ğŸ“š Citation

If you use this project in your research or teaching, please cite:

```bibtex
@software{code_llm_from_scratch,
  title={Code LLM from Scratch: Production Code Generation Models},
  author={Vijay Singh},
  year={2025},
  url={https://github.com/yourusername/code-llm-from-scratch}
}
```

## ğŸ¤ Contributing

We welcome contributions of all kinds! Whether you're:
- ğŸ› Reporting bugs
- âœ¨ Suggesting features
- ğŸ“ Improving documentation
- ğŸ’» Contributing code
- ğŸ§ª Adding tests
- ğŸ“Š Sharing datasets

Please see **[CONTRIBUTING.md](CONTRIBUTING.md)** for:
- Development setup
- Code style guide
- Testing requirements
- Pull request process
- Community guidelines

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- **CodeLlama** (Meta): Architecture inspiration
- **StarCoder** (Hugging Face): Training methodology
- **TinyStories** (Microsoft): Language dataset
- **PyTorch Team**: Framework and MPS backend

## ğŸ“ Contact

- **Author**: Vijay Singh
- **Email**: your.email@example.com
- **LinkedIn**: [Your Profile](https://linkedin.com/in/yourprofile)

---

**Built with â¤ï¸ for the ML community**

*Last updated: December 2025*
