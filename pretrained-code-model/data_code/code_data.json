{
  "scripts": [
    "#!/bin/bash\n# System health check script\n\necho \"=== System Health Report ===\"\necho \"Date: $(date)\"\necho\n\necho \"--- CPU Usage ---\"\ntop -bn1 | grep \"Cpu(s)\" | awk '{print \"CPU Usage: \" $2 + $4 \"%\"}'\n\necho\necho \"--- Memory Usage ---\"\nfree -h | grep Mem | awk '{print \"Total: \" $2 \", Used: \" $3 \", Free: \" $4}'\n\necho\necho \"--- Disk Usage ---\"\ndf -h | grep -vE '^Filesystem|tmpfs|cdrom'\n\necho\necho \"--- Top 5 Processes by CPU ---\"\nps aux --sort=-%cpu | head -6\n\necho\necho \"--- Network Connections ---\"\nnetstat -an | grep ESTABLISHED | wc -l | awk '{print \"Active connections: \" $1}'\n",
    "#!/bin/bash\n# Docker container cleanup script\n\necho \"Cleaning up Docker resources...\"\n\n# Remove stopped containers\necho \"Removing stopped containers...\"\ndocker container prune -f\n\n# Remove unused images\necho \"Removing dangling images...\"\ndocker image prune -f\n\n# Remove unused volumes\necho \"Removing unused volumes...\"\ndocker volume prune -f\n\n# Remove unused networks\necho \"Removing unused networks...\"\ndocker network prune -f\n\necho\necho \"Disk space reclaimed:\"\ndocker system df\n",
    "#!/bin/bash\n# Advanced log analyzer\n\nLOG_FILE=${1:-\"/var/log/syslog\"}\nOUTPUT_DIR=\"./log_analysis\"\n\nmkdir -p \"$OUTPUT_DIR\"\n\necho \"Analyzing $LOG_FILE...\"\n\n# Error count\necho \"--- Error Summary ---\" > \"$OUTPUT_DIR/errors.txt\"\ngrep -i \"error\" \"$LOG_FILE\" | cut -d':' -f4- | sort | uniq -c | sort -rn >> \"$OUTPUT_DIR/errors.txt\"\n\n# Warning count\necho \"--- Warning Summary ---\" > \"$OUTPUT_DIR/warnings.txt\"\ngrep -i \"warning\" \"$LOG_FILE\" | cut -d':' -f4- | sort | uniq -c | sort -rn >> \"$OUTPUT_DIR/warnings.txt\"\n\n# Timeline\necho \"--- Hourly Activity ---\" > \"$OUTPUT_DIR/timeline.txt\"\nawk '{print $3}' \"$LOG_FILE\" | cut -d':' -f1 | sort | uniq -c >> \"$OUTPUT_DIR/timeline.txt\"\n\necho \"Analysis complete. Results in $OUTPUT_DIR/\"\n",
    "#!/bin/bash\n# Advanced backup script with rotation\n\nSOURCE_DIR=\"${1:-/data}\"\nBACKUP_ROOT=\"/backup\"\nRETENTION_DAYS=7\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_DIR=\"$BACKUP_ROOT/$DATE\"\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n# Backup with progress\necho \"Starting backup of $SOURCE_DIR...\"\ntar -czf \"$BACKUP_DIR/backup.tar.gz\" \"$SOURCE_DIR\" 2>&1 |     while read line; do\n        echo \"  $line\"\n    done\n\n# Calculate checksum\ncd \"$BACKUP_DIR\"\nsha256sum backup.tar.gz > backup.tar.gz.sha256\n\n# Remove old backups\necho \"Cleaning old backups (older than $RETENTION_DAYS days)...\"\nfind \"$BACKUP_ROOT\" -maxdepth 1 -type d -mtime +$RETENTION_DAYS -exec rm -rf {} \\;\n\n# Report\nBACKUP_SIZE=$(du -sh \"$BACKUP_DIR\" | cut -f1)\necho \"Backup complete!\"\necho \"  Location: $BACKUP_DIR\"\necho \"  Size: $BACKUP_SIZE\"\necho \"  Checksum: $(cat backup.tar.gz.sha256)\"\n",
    "#!/bin/bash\n# Batch git repository updater\n\nREPO_DIR=\"${1:-.}\"\n\necho \"Updating all git repositories in $REPO_DIR...\"\n\nfind \"$REPO_DIR\" -name \".git\" -type d | while read gitdir; do\n    repo=$(dirname \"$gitdir\")\n    echo\n    echo \"=== $(basename \"$repo\") ===\"\n    cd \"$repo\"\n\n    # Check for changes\n    if [[ -n $(git status -s) ]]; then\n        echo \"  \u26a0 Uncommitted changes\"\n        git status -s\n    else\n        # Pull updates\n        echo \"  Pulling updates...\"\n        git pull --rebase\n\n        if [ $? -eq 0 ]; then\n            echo \"  \u2713 Updated successfully\"\n        else\n            echo \"  \u2717 Update failed\"\n        fi\n    fi\ndone\n\necho\necho \"All repositories processed\"\n",
    "#!/bin/bash\n# PostgreSQL database maintenance\n\nDB_NAME=\"${1:-mydb}\"\nDB_USER=\"${2:-postgres}\"\n\necho \"=== Database Maintenance: $DB_NAME ===\"\n\n# Backup\necho \"Creating backup...\"\nBACKUP_FILE=\"backup_${DB_NAME}_$(date +%Y%m%d).sql\"\npg_dump -U \"$DB_USER\" \"$DB_NAME\" > \"$BACKUP_FILE\"\ngzip \"$BACKUP_FILE\"\necho \"  Backup: ${BACKUP_FILE}.gz\"\n\n# Vacuum\necho \"Running VACUUM...\"\npsql -U \"$DB_USER\" -d \"$DB_NAME\" -c \"VACUUM ANALYZE;\"\n\n# Reindex\necho \"Reindexing...\"\npsql -U \"$DB_USER\" -d \"$DB_NAME\" -c \"REINDEX DATABASE $DB_NAME;\"\n\n# Statistics\necho\necho \"=== Database Statistics ===\"\npsql -U \"$DB_USER\" -d \"$DB_NAME\" -c \"    SELECT schemaname, tablename,\n           pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\n    FROM pg_tables\n    WHERE schemaname NOT IN ('pg_catalog', 'information_schema')\n    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\n    LIMIT 10;\"\n\necho \"Maintenance complete\"\n",
    "#!/bin/bash\n# Simple website deployment script\n\nREPO_URL=\"${1}\"\nDEPLOY_DIR=\"/var/www/html\"\nSERVICE_NAME=\"nginx\"\n\nif [ -z \"$REPO_URL\" ]; then\n    echo \"Usage: $0 <git-repo-url>\"\n    exit 1\nfi\n\n# Create temp directory\nTEMP_DIR=$(mktemp -d)\ncd \"$TEMP_DIR\"\n\necho \"Cloning repository...\"\ngit clone \"$REPO_URL\" .\n\n# Run tests if available\nif [ -f \"package.json\" ]; then\n    echo \"Running tests...\"\n    npm install\n    npm test || { echo \"Tests failed!\"; exit 1; }\nfi\n\n# Backup current deployment\nif [ -d \"$DEPLOY_DIR\" ]; then\n    echo \"Backing up current deployment...\"\n    tar -czf \"/tmp/backup_$(date +%Y%m%d_%H%M%S).tar.gz\" \"$DEPLOY_DIR\"\nfi\n\n# Deploy\necho \"Deploying...\"\nrsync -av --delete \"$TEMP_DIR/\" \"$DEPLOY_DIR/\"\n\n# Restart service\necho \"Restarting $SERVICE_NAME...\"\nsystemctl restart \"$SERVICE_NAME\"\n\n# Cleanup\ncd /\nrm -rf \"$TEMP_DIR\"\n\necho \"Deployment complete!\"\n",
    "#!/bin/bash\n# Server monitoring with alerts\n\nTHRESHOLD_CPU=80\nTHRESHOLD_MEM=90\nTHRESHOLD_DISK=85\nALERT_EMAIL=\"admin@example.com\"\n\nsend_alert() {\n    local subject=\"$1\"\n    local message=\"$2\"\n    echo \"$message\" | mail -s \"$subject\" \"$ALERT_EMAIL\"\n}\n\n# Check CPU\ncpu_usage=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | cut -d'%' -f1 | cut -d'.' -f1)\nif [ $cpu_usage -gt $THRESHOLD_CPU ]; then\n    send_alert \"High CPU Usage Alert\" \"CPU usage is ${cpu_usage}%\"\nfi\n\n# Check Memory\nmem_usage=$(free | grep Mem | awk '{print ($3/$2) * 100.0}' | cut -d'.' -f1)\nif [ $mem_usage -gt $THRESHOLD_MEM ]; then\n    send_alert \"High Memory Usage Alert\" \"Memory usage is ${mem_usage}%\"\nfi\n\n# Check Disk\ndisk_usage=$(df -h / | tail -1 | awk '{print $5}' | cut -d'%' -f1)\nif [ $disk_usage -gt $THRESHOLD_DISK ]; then\n    send_alert \"High Disk Usage Alert\" \"Disk usage is ${disk_usage}%\"\nfi\n\necho \"Monitoring complete at $(date)\"\n",
    "#!/bin/bash\n# API endpoint health checker\n\nENDPOINTS=(\n    \"https://api.example.com/health\"\n    \"https://api.example.com/status\"\n    \"https://api.example.com/version\"\n)\n\nTIMEOUT=10\nREPORT_FILE=\"api_health_$(date +%Y%m%d_%H%M%S).txt\"\n\n{\n    echo \"API Health Check Report\"\n    echo \"=======================\"\n    echo \"Date: $(date)\"\n    echo\n\n    for endpoint in \"${ENDPOINTS[@]}\"; do\n        echo \"Testing: $endpoint\"\n\n        start_time=$(date +%s%N)\n        response=$(curl -s -w \"HTTP_CODE:%{http_code}\" --max-time $TIMEOUT \"$endpoint\")\n        end_time=$(date +%s%N)\n\n        http_code=$(echo \"$response\" | grep -o \"HTTP_CODE:[0-9]*\" | cut -d':' -f2)\n        response_time=$(( (end_time - start_time) / 1000000 ))\n\n        if [ \"$http_code\" = \"200\" ]; then\n            echo \"  \u2713 Status: OK\"\n        else\n            echo \"  \u2717 Status: Failed (HTTP $http_code)\"\n        fi\n\n        echo \"  Response time: ${response_time}ms\"\n        echo\n    done\n\n    echo \"Report complete\"\n} | tee \"$REPORT_FILE\"\n",
    "#!/bin/bash\n# Simple process manager\n\nPROCESS_NAME=$1\nACTION=$2\nPID_FILE=\"/var/run/${PROCESS_NAME}.pid\"\n\nstart_process() {\n    if [ -f \"$PID_FILE\" ]; then\n        echo \"Process already running (PID: $(cat $PID_FILE))\"\n        return 1\n    fi\n\n    echo \"Starting $PROCESS_NAME...\"\n    nohup /usr/bin/$PROCESS_NAME > /dev/null 2>&1 &\n    echo $! > \"$PID_FILE\"\n    echo \"Started with PID: $(cat $PID_FILE)\"\n}\n\nstop_process() {\n    if [ ! -f \"$PID_FILE\" ]; then\n        echo \"Process not running\"\n        return 1\n    fi\n\n    PID=$(cat \"$PID_FILE\")\n    echo \"Stopping $PROCESS_NAME (PID: $PID)...\"\n    kill \"$PID\"\n\n    # Wait for process to stop\n    timeout=10\n    while kill -0 \"$PID\" 2>/dev/null && [ $timeout -gt 0 ]; do\n        sleep 1\n        timeout=$((timeout - 1))\n    done\n\n    if kill -0 \"$PID\" 2>/dev/null; then\n        echo \"Force killing process...\"\n        kill -9 \"$PID\"\n    fi\n\n    rm -f \"$PID_FILE\"\n    echo \"Stopped\"\n}\n\nstatus_process() {\n    if [ -f \"$PID_FILE\" ]; then\n        PID=$(cat \"$PID_FILE\")\n        if kill -0 \"$PID\" 2>/dev/null; then\n            echo \"$PROCESS_NAME is running (PID: $PID)\"\n            return 0\n        else\n            echo \"$PROCESS_NAME is not running (stale PID file)\"\n            return 1\n        fi\n    else\n        echo \"$PROCESS_NAME is not running\"\n        return 1\n    fi\n}\n\ncase \"$ACTION\" in\n    start)\n        start_process\n        ;;\n    stop)\n        stop_process\n        ;;\n    restart)\n        stop_process\n        sleep 2\n        start_process\n        ;;\n    status)\n        status_process\n        ;;\n    *)\n        echo \"Usage: $0 <process-name> {start|stop|restart|status}\"\n        exit 1\n        ;;\nesac\n",
    "#!/bin/bash\n# Production backup script with error handling and notification\nset -euo pipefail\n\nBACKUP_SOURCE=\"${1:-/data}\"\nBACKUP_DEST=\"${2:-/backup}\"\nDATE=$(date +%Y%m%d_%H%M%S)\nLOG_FILE=\"/var/log/backup_${DATE}.log\"\n\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $*\" | tee -a \"$LOG_FILE\"\n}\n\ncleanup() {\n    local exit_code=$?\n    if [ $exit_code -ne 0 ]; then\n        log \"ERROR: Backup failed with exit code $exit_code\"\n        # Send notification\n        echo \"Backup failed\" | mail -s \"Backup Alert\" admin@example.com\n    fi\n    exit $exit_code\n}\n\ntrap cleanup EXIT\n\nlog \"Starting backup of $BACKUP_SOURCE\"\n\n# Create backup directory\nmkdir -p \"$BACKUP_DEST\"\n\n# Perform backup\ntar -czf \"$BACKUP_DEST/backup_${DATE}.tar.gz\" \"$BACKUP_SOURCE\" 2>&1 | tee -a \"$LOG_FILE\"\n\n# Verify backup\nif tar -tzf \"$BACKUP_DEST/backup_${DATE}.tar.gz\" > /dev/null 2>&1; then\n    log \"Backup verified successfully\"\nelse\n    log \"ERROR: Backup verification failed\"\n    exit 1\nfi\n\n# Cleanup old backups (keep last 7 days)\nfind \"$BACKUP_DEST\" -name \"backup_*.tar.gz\" -mtime +7 -delete\n\nlog \"Backup completed successfully\"\n",
    "#!/bin/bash\n# Collect system metrics and generate report\n\nREPORT_DIR=\"/var/reports\"\nDATE=$(date +%Y%m%d)\nREPORT_FILE=\"$REPORT_DIR/system_metrics_$DATE.json\"\n\nmkdir -p \"$REPORT_DIR\"\n\n# Collect metrics\nCPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | cut -d'%' -f1)\nMEM_TOTAL=$(free -m | awk 'NR==2{print $2}')\nMEM_USED=$(free -m | awk 'NR==2{print $3}')\nMEM_PCT=$(awk \"BEGIN {printf \"%.2f\", ($MEM_USED/$MEM_TOTAL)*100}\")\n\nDISK_USAGE=$(df -h / | awk 'NR==2{print $5}' | sed 's/%//')\nLOAD_AVG=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')\n\n# Network stats\nRX_BYTES=$(cat /sys/class/net/eth0/statistics/rx_bytes 2>/dev/null || echo 0)\nTX_BYTES=$(cat /sys/class/net/eth0/statistics/tx_bytes 2>/dev/null || echo 0)\n\n# Generate JSON report\ncat > \"$REPORT_FILE\" <<EOF\n{\n  \"timestamp\": \"$(date -Iseconds)\",\n  \"hostname\": \"$(hostname)\",\n  \"metrics\": {\n    \"cpu_usage_percent\": $CPU_USAGE,\n    \"memory\": {\n      \"total_mb\": $MEM_TOTAL,\n      \"used_mb\": $MEM_USED,\n      \"usage_percent\": $MEM_PCT\n    },\n    \"disk_usage_percent\": $DISK_USAGE,\n    \"load_average\": $LOAD_AVG,\n    \"network\": {\n      \"rx_bytes\": $RX_BYTES,\n      \"tx_bytes\": $TX_BYTES\n    }\n  }\n}\nEOF\n\necho \"Metrics saved to $REPORT_FILE\"\n\n# Alert if thresholds exceeded\nif (( $(echo \"$CPU_USAGE > 80\" | bc -l) )); then\n    echo \"ALERT: High CPU usage: ${CPU_USAGE}%\"\nfi\n\nif (( $(echo \"$MEM_PCT > 90\" | bc -l) )); then\n    echo \"ALERT: High memory usage: ${MEM_PCT}%\"\nfi\n",
    "#!/bin/bash\n# Intelligent log rotation with analysis\n\nLOG_DIR=\"/var/log/app\"\nARCHIVE_DIR=\"/var/log/archive\"\nMAX_SIZE_MB=100\nRETENTION_DAYS=30\n\nrotate_logs() {\n    local log_file=\"$1\"\n    local size_mb=$(du -m \"$log_file\" | cut -f1)\n\n    if [ $size_mb -gt $MAX_SIZE_MB ]; then\n        echo \"Rotating $log_file (${size_mb}MB)\"\n\n        # Create archive directory\n        mkdir -p \"$ARCHIVE_DIR\"\n\n        # Compress and move\n        gzip -c \"$log_file\" > \"$ARCHIVE_DIR/$(basename $log_file)_$(date +%Y%m%d_%H%M%S).gz\"\n\n        # Truncate original\n        > \"$log_file\"\n\n        echo \"Rotated to archive\"\n    fi\n}\n\n# Analyze logs before rotation\nanalyze_logs() {\n    local log_file=\"$1\"\n\n    echo \"Log Analysis: $(basename $log_file)\"\n    echo \"  Total lines: $(wc -l < \"$log_file\")\"\n    echo \"  Errors: $(grep -c ERROR \"$log_file\" || echo 0)\"\n    echo \"  Warnings: $(grep -c WARN \"$log_file\" || echo 0)\"\n    echo\n\n    # Extract top errors\n    echo \"  Top errors:\"\n    grep ERROR \"$log_file\" | sort | uniq -c | sort -rn | head -3 || echo \"    None\"\n}\n\n# Process all log files\nfor log_file in \"$LOG_DIR\"/*.log; do\n    if [ -f \"$log_file\" ]; then\n        analyze_logs \"$log_file\"\n        rotate_logs \"$log_file\"\n    fi\ndone\n\n# Cleanup old archives\nfind \"$ARCHIVE_DIR\" -name \"*.gz\" -mtime +$RETENTION_DAYS -delete\n\necho \"Log rotation complete\"\n",
    "#!/bin/bash\n# Zero-downtime deployment with health checks\n\nAPP_NAME=\"myapp\"\nDEPLOY_DIR=\"/opt/$APP_NAME\"\nNEW_VERSION=\"$1\"\nHEALTH_URL=\"http://localhost:8080/health\"\nMAX_RETRIES=30\n\nif [ -z \"$NEW_VERSION\" ]; then\n    echo \"Usage: $0 <version>\"\n    exit 1\nfi\n\nhealth_check() {\n    local retries=0\n\n    while [ $retries -lt $MAX_RETRIES ]; do\n        if curl -sf \"$HEALTH_URL\" > /dev/null; then\n            echo \"Health check passed\"\n            return 0\n        fi\n\n        echo \"Health check attempt $((retries + 1))/$MAX_RETRIES...\"\n        sleep 2\n        retries=$((retries + 1))\n    done\n\n    echo \"Health check failed after $MAX_RETRIES attempts\"\n    return 1\n}\n\nrollback() {\n    echo \"Rolling back to previous version...\"\n    systemctl stop \"$APP_NAME\"\n    mv \"$DEPLOY_DIR/current\" \"$DEPLOY_DIR/failed_$NEW_VERSION\"\n    mv \"$DEPLOY_DIR/previous\" \"$DEPLOY_DIR/current\"\n    systemctl start \"$APP_NAME\"\n    echo \"Rollback complete\"\n}\n\n# Backup current version\nif [ -d \"$DEPLOY_DIR/current\" ]; then\n    mv \"$DEPLOY_DIR/current\" \"$DEPLOY_DIR/previous\"\nfi\n\n# Deploy new version\necho \"Deploying version $NEW_VERSION...\"\ngit clone --branch \"$NEW_VERSION\" https://github.com/user/app.git \"$DEPLOY_DIR/current\"\n\ncd \"$DEPLOY_DIR/current\"\n\n# Build\necho \"Building...\"\nmake build || { rollback; exit 1; }\n\n# Restart service\necho \"Restarting service...\"\nsystemctl restart \"$APP_NAME\"\n\n# Health check\necho \"Performing health check...\"\nif health_check; then\n    echo \"Deployment successful!\"\n    rm -rf \"$DEPLOY_DIR/previous\"\nelse\n    rollback\n    exit 1\nfi\n",
    "#!/bin/bash\n# Automated database maintenance\n\nDB_NAME=\"${1:-production}\"\nDB_USER=\"${2:-admin}\"\nBACKUP_DIR=\"/var/backups/db\"\nDATE=$(date +%Y%m%d)\n\nmkdir -p \"$BACKUP_DIR\"\n\n# Backup\necho \"Creating backup...\"\npg_dump -U \"$DB_USER\" \"$DB_NAME\" | gzip > \"$BACKUP_DIR/${DB_NAME}_${DATE}.sql.gz\"\n\n# Verify backup\nif gunzip -t \"$BACKUP_DIR/${DB_NAME}_${DATE}.sql.gz\"; then\n    echo \"Backup verified: $BACKUP_DIR/${DB_NAME}_${DATE}.sql.gz\"\nelse\n    echo \"Backup verification failed!\"\n    exit 1\nfi\n\n# Maintenance\necho \"Running VACUUM ANALYZE...\"\npsql -U \"$DB_USER\" -d \"$DB_NAME\" -c \"VACUUM ANALYZE;\"\n\n# Reindex\necho \"Reindexing...\"\npsql -U \"$DB_USER\" -d \"$DB_NAME\" -c \"REINDEX DATABASE $DB_NAME;\"\n\n# Check for bloat\necho \"Checking for table bloat...\"\npsql -U \"$DB_USER\" -d \"$DB_NAME\" -c \"\nSELECT\n  schemaname || '.' || tablename AS table,\n  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\nFROM pg_tables\nWHERE schemaname NOT IN ('pg_catalog', 'information_schema')\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 10;\n\"\n\n# Cleanup old backups\nfind \"$BACKUP_DIR\" -name \"${DB_NAME}_*.sql.gz\" -mtime +7 -delete\n\necho \"Database maintenance complete\"\n"
  ],
  "count": 15,
  "source": "comprehensive"
}