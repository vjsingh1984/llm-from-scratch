# LLM Training Experiments & Optimizations

This directory contains isolated experiments exploring optimizations and emerging research in LLM training.

## Directory Structure

```
experiments/
â”œâ”€â”€ optimizations/              # Performance improvements
â”‚   â”œâ”€â”€ gradient-checkpointing/ # Memory-compute trade-off
â”‚   â”œâ”€â”€ mixed-precision/        # FP16 training
â”‚   â”œâ”€â”€ flash-attention/        # Efficient attention
â”‚   â”œâ”€â”€ rope-embeddings/        # Better position encoding
â”‚   â”œâ”€â”€ swiglu/                 # Better activation
â”‚   â”œâ”€â”€ rmsnorm/                # Faster normalization
â”‚   â””â”€â”€ gradient-accumulation/  # Larger effective batch size
â”‚
â””â”€â”€ emerging-research/          # Cutting-edge approaches
    â”œâ”€â”€ mamba-ssm/             # Linear-time alternative to attention
    â”œâ”€â”€ mixture-of-experts/     # Sparse models
    â”œâ”€â”€ retrieval-augmented/    # External knowledge
    â”œâ”€â”€ test-time-compute/      # Inference-time optimization
    â”œâ”€â”€ long-context/           # Beyond transformer limits
    â””â”€â”€ efficient-architectures/ # RetNet, RWKV, Hyena

```

## Quick Reference

### ðŸŽ¯ By Impact (High â†’ Low)

**Memory Savings**:
1. ðŸ¥‡ Gradient Checkpointing (40-50% less memory)
2. ðŸ¥ˆ Mixed Precision (30-40% less memory)
3. ðŸ¥‰ Flash Attention (5-20x less memory for long sequences)

**Speed Improvements**:
1. ðŸ¥‡ Flash Attention (2-4x faster)
2. ðŸ¥ˆ torch.compile (20-30% faster)
3. ðŸ¥‰ Mixed Precision (2x faster)

**Quality Improvements**:
1. ðŸ¥‡ RoPE (better position encoding)
2. ðŸ¥ˆ SwiGLU (better activation)
3. ðŸ¥‰ Gradient Accumulation (larger effective batch size)

### ðŸ“Š By Difficulty (Easy â†’ Hard)

**Easy** (< 1 hour):
- torch.compile
- Gradient Accumulation
- RMSNorm

**Medium** (2-4 hours):
- Gradient Checkpointing
- Mixed Precision (AMP)
- RoPE
- SwiGLU

**Hard** (1-2 days):
- Flash Attention
- Grouped Query Attention
- Mixture of Experts
- Mamba

### ðŸŽ“ Educational Value

**Fundamentals**:
- Gradient Accumulation â†’ Batch size concepts
- Mixed Precision â†’ Numerical precision
- Gradient Checkpointing â†’ Memory-compute trade-offs

**Advanced**:
- Flash Attention â†’ Hardware-aware algorithms
- RoPE â†’ Position encoding theory
- Mamba â†’ Alternatives to attention

**Research Frontiers**:
- MoE â†’ Sparse computation
- RAG â†’ External knowledge integration
- Test-time Compute â†’ Inference optimization

## Implementation Order

### Phase 1: Quick Wins (While Model Training)
**Time**: 2-3 hours
**Goal**: Immediate improvements without stopping training

1. âœ… **Document current optimizations**
   - Status: Complete
   - See: `docs/OPTIMIZATIONS.md`

2. âœ… **Add Gradient Accumulation**
   - Status: Complete
   - Time: 30 minutes
   - Impact: Better gradients, effective larger batch size
   - Usage: `--gradient-accumulation-steps 4`
   - See: `optimizations/gradient-accumulation/`

3. âœ… **Add torch.compile**
   - Status: Complete
   - Time: 5 minutes (one line!)
   - Impact: 20-30% speedup
   - Usage: `--use-compile`
   - Code: `model = torch.compile(model)`

4. âœ… **Implement RMSNorm**
   - Status: Complete
   - Time: 1 hour
   - Impact: 10-15% faster normalization
   - Usage: `--use-rmsnorm`
   - See: `optimizations/rmsnorm/`

### Phase 2: Architecture Improvements âœ… COMPLETE
**Status**: Implemented
**Goal**: Retrain with architectural improvements

5. âœ… **Add RoPE**
   - Status: Complete
   - Impact: Better position encoding
   - Usage: `--use-rope` (Stage 1 ONLY)
   - See: `optimizations/rope-embeddings/`

6. âœ… **Implement Gradient Checkpointing**
   - Status: Complete
   - Impact: Fit larger models (40-50% less memory)
   - Usage: `--use-gradient-checkpointing`
   - See: `optimizations/gradient-checkpointing/`

7. âœ… **Add Mixed Precision**
   - Status: Complete
   - Impact: Faster training (2x speedup + memory savings)
   - Usage: `--use-amp`
   - See: `optimizations/mixed-precision/`

8. ðŸ”„ **Add SwiGLU**
   - Status: Planned
   - Impact: Better activation function
   - See: `optimizations/swiglu/`

### Phase 3: Research Exploration
**Time**: 1-2 weeks
**Goal**: Understand emerging approaches

9. ðŸ”¬ **Explore Mamba**
   - Learn: Linear-time sequence modeling
   - See: `emerging-research/mamba-ssm/`

10. ðŸ”¬ **Explore MoE**
    - Learn: Sparse computation
    - See: `emerging-research/mixture-of-experts/`

11. ðŸ”¬ **Explore RAG**
    - Learn: External knowledge integration
    - See: `emerging-research/retrieval-augmented/`

## Current Baseline

### Model: Medium (371M params)
- **Layers**: 24
- **Hidden**: 1024
- **Sequence**: 1024 tokens
- **Memory**: ~6-8GB
- **Speed**: TBD (training in progress)
- **Quality**: TBD (training in progress)

### Optimizations Active
- âœ… Learning rate warmup
- âœ… Cosine LR decay
- âœ… Gradient clipping (1.0)
- âœ… NaN detection
- âœ… Loss clamping
- âœ… AdamW optimizer
- âœ… Weight tying
- âœ… Pre-LayerNorm
- âœ… Dropout (0.1)
- âœ… CPU data loading
- âœ… MPS cache clearing

## Benchmarking Template

For each experiment:

### 1. Before Implementation
```markdown
## Baseline Metrics
- Memory: X GB
- Speed: Y tokens/sec
- Loss: Z.ZZ
- Training time: H hours
```

### 2. After Implementation
```markdown
## Results
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Memory | XGB | YGB | +/-Z% |
| Speed | X tok/s | Y tok/s | +Z% |
| Loss | X.XX | Y.YY | +/-Z% |
| Time | Xh | Yh | +/-Z% |
```

### 3. Analysis
```markdown
## Findings
- What worked: [details]
- What didn't: [details]
- Surprises: [details]
- Recommendations: [when to use, when to skip]
```

## Learning Objectives

By the end of these experiments, you'll understand:

### Technical
1. **Memory-Compute Trade-offs** (Gradient Checkpointing)
2. **Hardware-Aware Algorithms** (Flash Attention)
3. **Numerical Precision** (Mixed Precision)
4. **Position Encoding** (RoPE vs Learned)
5. **Activation Functions** (GELU vs SwiGLU)
6. **Normalization** (LayerNorm vs RMSNorm)
7. **Attention Alternatives** (Mamba, SSMs)
8. **Sparse Models** (MoE)

### Conceptual
1. **No free lunch** - Every optimization has trade-offs
2. **Measure everything** - Intuition can be wrong
3. **Context matters** - What works for GPT-4 may not work for your model
4. **Emerging is risky** - Cutting-edge has rough edges
5. **Fundamentals matter** - Good data beats fancy architecture

## Resources

### Papers (Essential)
- "Attention Is All You Need" (Vaswani et al., 2017) - The foundation
- "GPT-3" (Brown et al., 2020) - Scaling laws
- "LLaMA" (Touvron et al., 2023) - Modern architecture choices
- "Flash Attention" (Dao et al., 2022) - Efficient attention
- "Mamba" (Gu & Dao, 2023) - Attention alternative

### Code References
- Hugging Face Transformers - Production implementations
- nanoGPT (Karpathy) - Educational Transformer
- PyTorch Examples - Official tutorials
- flash-attn - Efficient attention kernels

### Courses
- Stanford CS224N - NLP with Deep Learning
- Fast.ai - Practical Deep Learning
- Andrej Karpathy - Neural Networks: Zero to Hero

## Contributing

Each experiment should include:
1. **README.md** - Overview, theory, implementation plan
2. **Code** - Working implementation
3. **Results** - Benchmark data, plots
4. **Analysis** - What we learned

## Status Tracker

| Experiment | Status | Priority | Difficulty | Impact | Stage Compatibility |
|-----------|--------|----------|------------|--------|-------------------|
| Gradient Accumulation | âœ… Complete | High | Easy | Medium | Stage 1 & 2 |
| torch.compile | âœ… Complete | High | Easy | High | Stage 1 & 2 |
| Mixed Precision (AMP) | âœ… Complete | High | Medium | High | Stage 1 & 2 |
| Gradient Checkpointing | âœ… Complete | High | Medium | High | Stage 1 & 2 |
| RMSNorm | âœ… Complete | Medium | Easy | Low | **Stage 1 ONLY** |
| RoPE | âœ… Complete | Medium | Medium | Medium | **Stage 1 ONLY** |
| SwiGLU | ðŸ“ Planned | Medium | Medium | Medium | **Stage 1 ONLY** |
| Flash Attention | ðŸ“ Planned | Low | Hard | High* | Stage 1 & 2 |
| Mamba | ðŸ“ Planned | Low | Hard | Educational | Stage 1 & 2 |
| MoE | ðŸ“ Planned | Low | Hard | Educational | Stage 1 & 2 |

*High impact but CUDA-only (not available on MPS)

### Key Notes:
- **Stage 1 ONLY**: Architecture-changing optimizations (RMSNorm, RoPE, SwiGLU) can't be used when loading checkpoints
- **Stage 1 & 2**: Training-only optimizations (compile, AMP, gradient checkpointing, gradient accumulation) work for both stages

## Questions to Explore

1. **Do modern optimizations help small models?** Most research focuses on billion+ parameter models. Do these techniques scale down?

2. **Is attention really necessary?** Mamba suggests no. When does it break down?

3. **What's the optimal sequence length?** We use 1024. Is that right for our use case?

4. **How much does architecture matter vs data?** If we improve the model 10%, is that as good as 10% more data?

5. **What optimizations stack?** Can we combine gradient checkpointing + mixed precision + flash attention?

---

**Remember**: The goal isn't just to train a model, but to deeply understand how modern LLMs work and where the field is heading. Each experiment is a learning opportunity!

**Last Updated**: 2024
