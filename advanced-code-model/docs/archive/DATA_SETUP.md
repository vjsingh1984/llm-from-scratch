# Data Setup Guide

**How to download and prepare training datasets**

---

## ğŸ“¦ **Quick Start (Automated)**

```bash
# Make script executable
chmod +x scripts/setup_data.sh

# Run setup
./scripts/setup_data.sh
```

This will automatically:
1. âœ… Download TinyStories dataset (~100MB)
2. âœ… Download bash scripts for code training
3. âœ… Train BPE tokenizer
4. âœ… Prepare all training data

**Time:** ~15-20 minutes
**Disk space:** ~500MB

---

## ğŸ“‹ **Manual Setup (Step-by-Step)**

### **Step 1: Download TinyStories Dataset**

```bash
# Create directories
mkdir -p data/raw
mkdir -p data/processed

# Download training data
curl -L "https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt" \
  -o "data/raw/TinyStoriesV2-GPT4-train.txt"

# Download validation data
curl -L "https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt" \
  -o "data/raw/TinyStoriesV2-GPT4-valid.txt"
```

**Size:** ~100MB compressed
**Source:** [TinyStories on HuggingFace](https://huggingface.co/datasets/roneneldan/TinyStories)

---

### **Step 2: Train Tokenizer**

```bash
# Train BPE tokenizer on TinyStories
python3 scripts/train_tokenizer.py
```

**Output:** `data/tokenizer/tokenizer.json`
**Vocabulary size:** 32,000 tokens
**Time:** ~2-3 minutes

---

### **Step 3: Prepare Language Data**

```bash
# Process TinyStories into training format
python3 scripts/download_data.py
```

**Output:**
- `data/processed/language_train.npy` (~400MB)
- `data/processed/language_val.npy` (~20MB)

**Time:** ~5-10 minutes

---

### **Step 4: Prepare Code Data (Optional)**

The code dataset is collected from bash script repositories. You have two options:

#### **Option A: Use Existing Data** (If Already Prepared)
If you already ran `download_data.py`, the code data should be ready.

#### **Option B: Re-download** (Fresh Setup)
```bash
python3 scripts/download_data.py
```

This will clone bash script repos and prepare code data.

**Output:**
- `data/processed/code_train.npy` (~9MB)
- `data/processed/code_val.npy` (~500KB)

**Time:** ~10-15 minutes

---

## ğŸ“Š **Expected Directory Structure**

After setup, you should have:

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ TinyStoriesV2-GPT4-train.txt      # ~100MB
â”‚   â”œâ”€â”€ TinyStoriesV2-GPT4-valid.txt      # ~5MB
â”‚   â””â”€â”€ bash/
â”‚       â””â”€â”€ raw/
â”‚           â””â”€â”€ repos/                     # Bash script repos (gitignored)
â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ tokenizer.json                     # ~3MB
â””â”€â”€ processed/
    â”œâ”€â”€ language_train.npy                 # ~400MB
    â”œâ”€â”€ language_val.npy                   # ~20MB
    â”œâ”€â”€ code_train.npy                     # ~9MB
    â””â”€â”€ code_val.npy                       # ~500KB
```

**Total size:** ~500-600MB

---

## âœ… **Verification**

Check that all files exist:

```bash
# Check raw data
ls -lh data/raw/*.txt

# Check tokenizer
ls -lh data/tokenizer/tokenizer.json

# Check processed data
ls -lh data/processed/*.npy
```

**Expected output:**
```
data/raw/TinyStoriesV2-GPT4-train.txt    ~100M
data/raw/TinyStoriesV2-GPT4-valid.txt    ~5M
data/tokenizer/tokenizer.json            ~3M
data/processed/language_train.npy        ~400M
data/processed/language_val.npy          ~20M
data/processed/code_train.npy            ~9M
data/processed/code_val.npy              ~500K
```

---

## ğŸ¯ **Stage-Specific Data**

### **Stages 1-2:** Already Prepared âœ…
- Language data (TinyStories)
- Code data (bash scripts)

### **Stage 3+:** Generate on Demand
```bash
# Stage 3: Tool calling data
python3 scripts/prepare_tool_calling_data.py

# Stage 4: RLHF preference data
python3 scripts/prepare_rlhf_data.py

# Stage 5: Multi-modal data
python3 scripts/prepare_multimodal_data.py

# Stage 8: Domain specialization data
python3 scripts/prepare_domain_data.py
```

These generate **synthetic data** (no downloads needed).

---

## ğŸ“¦ **Data Sources**

### **TinyStories** (Stage 1: Language)
- **Source:** [HuggingFace](https://huggingface.co/datasets/roneneldan/TinyStories)
- **License:** MIT
- **Size:** ~100MB
- **Description:** GPT-4 generated short stories for language pretraining

### **Bash Scripts** (Stage 2: Code)
- **Source:** Various GitHub repositories
- **Collection method:** `download_data.py` clones public repos
- **License:** Mixed (various open source licenses)
- **Size:** ~500MB raw, ~9MB processed
- **Description:** Real-world bash scripts for code fine-tuning

### **Synthetic Data** (Stages 3+)
- **Generated by:** Our preparation scripts
- **No download required**
- **Examples:** Tool calling, RLHF preferences, multi-modal pairs

---

## ğŸ”„ **Re-downloading Data**

If you need to re-download or regenerate:

```bash
# Remove existing data
rm -rf data/raw/*
rm -rf data/processed/*

# Re-run setup
./scripts/setup_data.sh

# Or manually:
python3 scripts/download_data.py
```

---

## âš ï¸ **Troubleshooting**

### **Download fails:**
```bash
# Check internet connection
ping huggingface.co

# Try manual download
# Visit: https://huggingface.co/datasets/roneneldan/TinyStories
```

### **Tokenizer training fails:**
```bash
# Make sure TinyStories is downloaded first
ls data/raw/TinyStoriesV2-GPT4-train.txt

# Install tokenizers library
pip install tokenizers
```

### **Data preparation fails:**
```bash
# Check Python dependencies
pip install torch numpy tokenizers

# Check disk space
df -h .
```

---

## ğŸ’¾ **Disk Space Management**

**To save space, you can delete:**
- âœ… `data/raw/` (after processing) - Saves ~100MB
- âœ… `data/bash/raw/repos/` (after processing) - Saves ~400MB

**Keep:**
- âŒ `data/processed/*.npy` - Required for training!
- âŒ `data/tokenizer/` - Required for all stages!

```bash
# Safe cleanup (after data is prepared)
rm -rf data/raw/
rm -rf data/bash/raw/repos/
```

This reduces disk usage from ~600MB to ~450MB.

---

## ğŸš€ **Next Steps**

After data setup is complete:

1. **Verify data:**
   ```bash
   ls -lh data/processed/
   ```

2. **Start training:**
   ```bash
   python3 scripts/train.py --stage language --model-size large
   ```

3. **Follow guide:**
   - [Step-by-Step Guide](STEP_BY_STEP_GUIDE.md)

---

## ğŸ“ **For Git Repository**

**Files committed to git:**
- âœ… Scripts to download data (`scripts/download_data.py`)
- âœ… Scripts to prepare data (`scripts/prepare_*_data.py`)
- âœ… Setup script (`scripts/setup_data.sh`)
- âœ… This documentation

**Files NOT committed (in .gitignore):**
- âŒ `data/raw/` - Large downloads
- âŒ `data/processed/*.npy` - Large processed files
- âŒ `models/*.pth` - Large model checkpoints

**Why?**
- Git is not designed for large binary files
- Each user downloads/generates their own data
- Keeps repository size small (~50MB vs ~5GB)
- Standard practice for ML projects

---

## ğŸŒ **Alternative: Pre-processed Data (Advanced)**

If you want to share pre-processed data with team members:

```bash
# Create archive (on source machine)
tar -czf llm-training-data.tar.gz data/processed/ data/tokenizer/

# Share via cloud storage (Google Drive, S3, etc.)

# Extract (on target machine)
tar -xzf llm-training-data.tar.gz
```

**Filesize:** ~450MB compressed

---

**Ready to start training!** ğŸš€

See [STEP_BY_STEP_GUIDE.md](STEP_BY_STEP_GUIDE.md) for next steps.
