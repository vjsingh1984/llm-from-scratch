# Pretrain on Language, Then Fine-tune on Code

## Why This Approach Works Better

### Traditional Code Model Training

```
Code-only training:
- Model learns bash syntax ✓
- Model CANNOT understand "write a script to backup files" ✗
- Model CANNOT explain what code does ✗
- Limited by code-only vocabulary ✗
```

### Modern Approach (CodeLlama, StarCoder, etc.)

```
Stage 1: Pretrain on Natural Language
  ↓
  Model learns:
  - English vocabulary and grammar
  - Reasoning and logic
  - Common knowledge
  - Instruction following

Stage 2: Fine-tune on Code
  ↓
  Model learns:
  - Programming syntax
  - Code patterns
  - Algorithm implementation
  - But retains language understanding!

Result:
  - Responds to English prompts ✓
  - Generates code from descriptions ✓
  - Explains code in English ✓
  - Best of both worlds ✓
```

## Real-World Example

### Code-Only Model

```
Input: "write a script to list files"
Output: [Random bash tokens - doesn't understand the request]
```

### Language+Code Model

```
Input: "write a script to list files"
Output:
#!/bin/bash
# List all files in current directory
ls -la
```

The model understands:
- "write" = generate code
- "script" = bash script
- "list files" = use `ls` command

## How Modern Code Models Are Built

### 1. CodeLlama (Meta)

```
Llama 2 (70B params, trained on 2T tokens of text)
    ↓
Continue training on 500B tokens of code
    ↓
CodeLlama (understands both language and code)
```

### 2. StarCoder (Hugging Face)

```
Stage 1: Train on The Stack (6TB of code)
Stage 2: Instruction fine-tuning
    ↓
StarCoder (code generation + chat)
```

### 3. Our Approach (Simplified)

```
Stage 1: Pretrain tiny model on English text (TinyStories)
    ↓
    - 10M parameter model
    - 5000 stories
    - Learns language patterns

Stage 2: Continue training on bash scripts
    ↓
    - Same model, continue from checkpoint
    - Add bash scripts to training
    - Model combines language + code knowledge

Result: Bash Coder that understands English prompts!
```

## Data Sources for English Pretraining

### 1. TinyStories (Recommended for Small Models)

**What:** Synthetic stories generated by GPT-3.5/GPT-4
**Size:** 2.1M stories, ~430MB
**Why Good:**
- Simple, clean language
- Perfect for small models
- Fast to train
- We already have this!

**Download:**
```python
from datasets import load_dataset

dataset = load_dataset("roneneldan/TinyStories")
# Save to files for training
```

**Already Used In:** Our MLX LLM project! (`llm-from-scratch/data/tiny_stories.txt`)

### 2. Wikipedia (English)

**What:** Wikipedia articles
**Size:** ~20GB compressed, ~70GB uncompressed
**Why Good:**
- High quality, factual
- Diverse topics
- Well-formatted

**Download:**
```bash
# Latest Wikipedia dump
wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2

# Process with wikiextractor
pip install wikiextractor
wikiextractor enwiki-latest-pages-articles.xml.bz2 -o extracted/
```

**Processing Script:**
```python
import json

# Extract text from Wikipedia dump
wiki_texts = []
for line in open('wiki.json'):
    article = json.loads(line)
    wiki_texts.append(article['text'])
```

### 3. BookCorpus

**What:** 11,000 free books
**Size:** ~5GB
**Why Good:**
- Natural, long-form text
- Diverse writing styles
- Good for language modeling

**Download:**
```python
from datasets import load_dataset

dataset = load_dataset("bookcorpus")
```

### 4. C4 (Colossal Clean Crawled Corpus)

**What:** Cleaned web pages
**Size:** 750GB (but can use smaller subsets)
**Why Good:**
- Huge and diverse
- Already cleaned
- Used by T5, GPT-3

**Download (Subset):**
```python
from datasets import load_dataset

# Load small subset
dataset = load_dataset("c4", "en", split="train[:1%]")
```

### 5. OpenWebText

**What:** Recreation of GPT-2's WebText
**Size:** ~38GB
**Why Good:**
- Reddit links → high quality
- Similar to GPT-2 training data
- Open source

**Download:**
```python
from datasets import load_dataset

dataset = load_dataset("openwebtext")
```

### 6. The Pile (Comprehensive)

**What:** 825GB diverse dataset
**Components:**
- Books
- Wikipedia
- GitHub (code)
- arXiv (papers)
- StackExchange
- Many more

**Why Good:**
- Most comprehensive
- Used by EleutherAI GPT models
- Includes code already!

**Download (Subset):**
```python
from datasets import load_dataset

# The Pile is huge, download specific components
dataset = load_dataset("EleutherAI/pile", split="train", streaming=True)
```

### 7. Common Crawl (Advanced)

**What:** Petabytes of web pages
**Size:** Massive (petabytes)
**Why Good:**
- Most comprehensive web data
- Real-world language use

**Not Recommended For:**
- Small models (too large)
- Limited compute
- Requires heavy filtering

## Recommended Strategy for Our Project

### Option 1: Fastest (Use Existing TinyStories Model)

```bash
# We already have a model pretrained on TinyStories!
# Just continue training on bash scripts

cd llm-from-scratch
# Use the trained model from this project

cd ../bash-code-model
# Continue training on bash scripts
python scripts/train_continue.py \
    --checkpoint ../llm-from-scratch/checkpoints/checkpoint_step_5000.pt \
    --data-dir data_large
```

**Result:** Language+Code model in minutes!

### Option 2: Train from Scratch (Learning)

```
Stage 1: Pretrain on English (2-4 hours)
    ↓
    python scripts/train.py \
        --data-dir ../llm-from-scratch/data \
        --num-steps 5000 \
        --model-size tiny

Stage 2: Continue on Code (30 minutes)
    ↓
    python scripts/train_continue.py \
        --checkpoint checkpoints/language_model.pt \
        --data-dir data_large \
        --num-steps 1000

Result: Full understanding of both domains
```

### Option 3: Mixed Training (Best Quality)

```python
# Mix language and code in training data
# 80% English, 20% code

dataset = {
    'english': load_tinystories(),
    'code': load_bash_scripts(),
    'mixed': combine(english * 0.8, code * 0.2)
}

# Train on mixed data from start
train(dataset['mixed'])
```

## Practical Implementation

### Create Combined Dataset

```python
#!/usr/bin/env python3
"""
Create combined language + code dataset.
"""

from pathlib import Path
import json

def create_combined_dataset():
    # Load English data (TinyStories)
    english_texts = []
    stories_file = Path("../llm-from-scratch/data/tiny_stories.txt")

    if stories_file.exists():
        with open(stories_file) as f:
            english_texts = f.read().split('\n\n')

    # Load code data
    code_texts = []
    code_file = Path("data_large/bash_scripts.json")

    with open(code_file) as f:
        data = json.load(f)
        code_texts = data['scripts']

    # Combine with ratio (80% language, 20% code)
    # Repeat code samples to match ratio
    code_repeat = int(len(english_texts) * 0.2 / len(code_texts))

    combined = english_texts + (code_texts * code_repeat)

    # Shuffle
    import random
    random.shuffle(combined)

    # Save
    output_dir = Path("data_combined")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "combined.json", 'w') as f:
        json.dump({
            'texts': combined,
            'count': len(combined),
            'language_ratio': len(english_texts) / len(combined),
            'code_ratio': (len(code_texts) * code_repeat) / len(combined)
        }, f, indent=2)

    print(f"Combined dataset created:")
    print(f"  English samples: {len(english_texts)}")
    print(f"  Code samples: {len(code_texts) * code_repeat}")
    print(f"  Total: {len(combined)}")
    print(f"  Ratio: {len(english_texts)/len(combined):.1%} language, "
          f"{(len(code_texts)*code_repeat)/len(combined):.1%} code")

if __name__ == '__main__':
    create_combined_dataset()
```

## Vocabulary Considerations

### Code-Only Model

```
Vocabulary: ~100 characters
- Can represent code: ✓
- Can understand English prompts: ✗
```

### Language+Code Model

```
Vocabulary: ~8000 BPE tokens (or 256 characters for both)
- English words: "script", "backup", "files" ✓
- Code syntax: "#!/bin/bash", "if", "for" ✓
- Best of both worlds ✓
```

### Our Options

**Option A: Character-level (Current)**
- 102 tokens (letters, digits, symbols)
- Works for both English and code
- Longer sequences
- Simple to implement

**Option B: BPE with Code**
- Train BPE on combined English+Code corpus
- ~8000 tokens
- Efficient for both domains
- Used by CodeLlama, StarCoder

**Recommendation:** Start with character-level (simpler), upgrade to BPE if scaling up

## Example Training Pipeline

### 1. Download English Data

```bash
# Use TinyStories (we already have it!)
cd ../llm-from-scratch/data
ls tiny_stories.txt  # Already downloaded!
```

### 2. Prepare Combined Dataset

```bash
cd ../bash-code-model
python scripts/create_combined_dataset.py
```

### 3. Train Language Model First

```bash
python scripts/train.py \
    --data-dir data_combined \
    --model-size tiny \
    --num-steps 3000 \
    --output-dir checkpoints_language
```

### 4. Fine-tune on Code

```bash
python scripts/train_continue.py \
    --checkpoint checkpoints_language/best_model.pt \
    --data-dir data_large \
    --num-steps 1000 \
    --learning-rate 1e-4 \
    --output-dir checkpoints_final
```

### 5. Test Combined Model

```bash
python scripts/generate.py \
    --checkpoint checkpoints_final/best_model.pt \
    --prompt "Create a bash script to backup files"
```

**Expected Output:**
```bash
#!/bin/bash
# Backup script
tar -czf backup.tar.gz /data
echo "Backup complete"
```

## Comparison of Approaches

| Approach | Training Time | Quality | Prompts | Use Case |
|----------|---------------|---------|---------|----------|
| Code Only | 30 min | Good syntax | No | Learn transformers |
| Language → Code | 3-4 hours | Best | Yes | Production |
| Mixed Training | 2-3 hours | Very Good | Yes | Balanced |

## Next Steps

### Immediate: Use Our Existing Work!

We already have a language model trained on TinyStories in the `llm-from-scratch` project. We can:

1. **Copy that checkpoint**
2. **Continue training on bash scripts**
3. **Get a bilingual model (English + Bash)!**

### Implementation

```bash
# Create training script that uses both projects
python scripts/train_bilingual.py \
    --language-checkpoint ../llm-from-scratch/checkpoints/checkpoint_step_5000.pt \
    --code-data data_large \
    --output-dir checkpoints_bilingual
```

## Conclusion

**Key Insights:**

1. ✅ **Pretrain on language** → Model understands English
2. ✅ **Fine-tune on code** → Model learns programming
3. ✅ **Result** → Responds to English prompts with code!

**Best Data Sources:**

- **Tiny Models (<50M):** TinyStories (we have this!)
- **Small Models (50-200M):** Wikipedia + BookCorpus
- **Medium Models (200M-1B):** OpenWebText + The Pile
- **Large Models (>1B):** C4 + The Pile + Common Crawl

**Our Recommendation:**

Start with what we have! We already trained a model on TinyStories. Just continue training it on bash scripts, and you'll have a model that:
- Understands English prompts
- Generates bash code
- Can explain what code does

This is exactly how real code LLMs are built, just at a smaller scale for learning!

---

**Further Reading:**
- CodeLlama paper: https://arxiv.org/abs/2308.12950
- StarCoder paper: https://arxiv.org/abs/2305.06161
- The Pile paper: https://arxiv.org/abs/2101.00027
